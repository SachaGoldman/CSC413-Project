\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}

\usepackage[final]{proposal_neurips_2021}

\bibliographystyle{abbrvnat}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}

\title{Proposal: DINO Methods for Style Transfer}

\author{
  Sacha Goldman \\
  Department of Computer Science\\ 
  Department of Mathematics\\
  University of Toronto\\
  Toronto, ON M5S 1A1 \\
  \texttt{sacha.goldman@mail.utoronto.ca} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
Style transfer is the problem of changing an image's artistic style while maintaining its ``content'', namely the object it depicts. Conventional models use CNN to extract features from an image, take in another image indicating the desired artistic style, and attempt to apply that style to the features of the original input image. However, it has been shown that due to the locality resulting from shared weights, CNN struggles to capture global information of input images and suffers from feature losses \cite{ImageStyleTransformer}. To resolve these issues, models that use Transformers instead of CNN for feature and style extraction have been proposed and shown improved results \cite{ImageStyleTransformer}. The authors of \cite{ImageStyleTransformer} use 


\section{Related Works}

\cite{dino} came up with a new self-supervised learning method, named DINO (for Knowledge Distillation with No Labels) that aims to create better feature representations from its inputs by adopting a teacher-student approach. The student gets a difficult version (lots of augmentation) of the image while the teacher gets an easy version (minimal augmentation), and then the models are trained to aim for the same feature representation through cross entropy loss. Their experiment results found that the vision transformer's class token contained explicit information about the segmentation of objects in images, while also performing very well as classifiers. Our motivation for this project stems from how this DINO method has shown to have been able to achieve a better feature representation than supervised vision transformers, and we would like to apply this in a new domain: image style transfer.

Self-supervised learning has been a common technique used in style transfer, due to the obvious lack of target images available for a task like this. It becomes necessary to find ways for the model to supervise its own outputs. Usual methods (add sources) involve checking the outputs of VGG for consistency in style and context. 



(write something about the Style Transfer Transformer)

(write something about different style transfer methods)


\section{Method}

We have four different idea for brining the success of the DINO method to the domain of style transfer.

\subsection{DINO Content Encoder}

We replace the content encoder with a transformer trained by DINO and then train the style and decoders using the regular style transfer paradigm. The idea being that the DINO models robust features vectors may act as powerful abstractions of the information that can be decoded with the new style.

\subsection{Pre-trained DINO Content Encoder}

\subsection{DINO Training for Encoders}

\subsection{Modified DINO Training for Encoders}




\medskip

% Our current sources
\nocite{*}

\bibliography{bib}

\end{document}