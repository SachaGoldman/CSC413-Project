\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}

\usepackage[preprint]{neurips_2021}

\bibliographystyle{abbrvnat}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{scrextend}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{bm}

\title{DINO Methods for Style Transfer}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
	Sacha Goldman \\
	Department of Computer Science\\ 
	Department of Mathematics\\
	University of Toronto\\
	Toronto, ON M5S 1A1 \\
	\texttt{sacha.goldman@mail.utoronto.ca} \\
	\And
	Yuchong Zhang \\
	Department of Computer Science \\
	Department of Mathematics \\
	University of Toronto \\
	Toronto, ON, M5S 1A1 \\
	\texttt{yuchongz.zhang@mail.utoronto.ca} \\
	\And
	Shirley Wang \\
	Department of Computer Science \\
	University of Toronto \\
	Toronto, ON M5S 1A1 \\
	\texttt{shirlsyuemeng.wang@mail.utoronto.ca} \\
}

\begin{document}

\maketitle

\begin{abstract}
We combine DINO \cite{DINO}, a self-supervised learning method that captures feature representations of input images, with StyTr2 \cite{ImageStyleTransformer}, a style transfer transformer that renders images with artistic style, and explore the nature and robustness of the resulting model.
\end{abstract}

\section{Introduction}
Style transfer is the problem of changing an image's artistic style while maintaining its ``content'', namely the object it depicts. Conventional models use a CNN to extract features from an image, take in another image indicating the desired artistic style, and attempt to apply that style to the features of the original input image. However, it has been shown that due to the locality resulting from shared weights, CNN based models struggle to capture global information of input images and suffer from feature losses \cite{ImageStyleTransformer}. To resolve these issues, models that use transformers instead of CNNs for feature and style extraction have been proposed and show better results \cite{ImageStyleTransformer}. The model StyTr2 proposed in \cite{ImageStyleTransformer} takes in a content image and a style image, uses two transformer encoders to capture the content and style, then renders the content with that style using a thrid transformer decoder. In our project, we will substitute the DINO method and models from \cite{DINO} for the encoder components and explore the effects on the task of style transfer. The DINO method is good at extracting content even if the input images are distorted or partial, so in particular, we will implement the models and conduct experiments to explore the following questions:
\begin{enumerate}
	\item By using DINO to extract content, can we obtain a style transfer that is more robust to distortion of the content image?
	\item Does DINO's ability to capture an image's global property from local pieces give it more robustness as a style encoder?
	\item Are the results of this new style transfer method better than previous methods?
\end{enumerate}

\section{Related Works}

\cite{DINO} came up with a new self-supervised learning method, named DINO (for Knowledge Distillation with No Labels) that aims to create better feature representations from its inputs by adopting a teacher-student paradigm. The student gets a difficult version (lots of augmentation) of the image while the teacher gets an easy version (minimal augmentation), and then the models are trained to aim for the same feature representation through cross entropy loss. Their experimental results found that the vision transformer's class token contained explicit information about the segmentation of objects in images, while also performing as very accurate classifiers. In this project, we aim to apply the better feature representation that the DINO method yields (over supervised vision transformers), to the domain of image style transfer.

Self-supervised learning is a common technique used in style transfer, due to the obvious lack of target images available for a task of this nature. It becomes necessary to find ways for the model to supervise its own outfputs. Usual methods \cite{CNNStyleTransfer} involve checking the outputs of VGG for consistency in style and context. \cite{ImageStyleTransformer} proposes using vision transformers for this task, using two transformer encoders (one for content, one for style), and one transformer decoder. Transformers have been a hot rising topic in computer vision, and they claim that their method of using transformers for style transfer results in a decrease in content leak, as well as achieves unbiased stylization.

It becomes integral for there to be good feature representations of the content and style to construct a good style-transferred image. We believe that making use of pretrained DINO ViT models, as well as using the DINO method itself during training, can help achieve this.

\section{Methods}

\subsection{DINO Pretrained ViT Content Encoder}

The DINO paper provided their Vision Transformers that were trained using their proposed method. We attempted to use these models as a starting point for the content and style encoders of the model in three different ways: A) Freezing the weights for both the content and style encoders, B) Freezing only the weights for the content encoder, and C) letting both the content and style encoders train with the rest of the network.

\subsection{DINO Training for Encoders}

To translate the success of DINO training on classification to style transfer we modify our algorithm and architecture. Now instead of simple a style encoder, there is a teacher style encoder $SE_t$ and a student style encoder $SE_s$, both using the architecture from \cite{ImageStyleTransformer}. This forces the students to learn distillations of the original images. Because style is a global property of an image, we applied this only to the style encoder and left the content encoder $CE$ as in \cite{ImageStyleTransformer}. 

To train this model, as in the \cite{DINO}, from the original style image we generate two slightly modified global views of the image $(I_s)^g_1$ and $(I_s)^g_2$ as well as a collection more heavily modified local views, with the set of all views denoted $V$. The student is passed all the views whereas the teacher is only passed the global views. We then calculate 
\begin{align}
	\mathcal L_d &= \sum_{I \in \{(I_s)^g_1, (I_s)^g_2\}} \sum_{I' \in V - \{ I \}} H(SE_t(I), SE_s(I'))
\end{align} 
Where $H(a,b) = -a \log (b)$. This loss is what forces our student style encoder to learn these distilled representations. We can average $SE_s((I_s)^g_1)$ and $SE_s(I_s)^g_2$ to get the encoded style, and we can take $CE(I_c)$ to get the content. From this we can generate $I_o$, the output image, using the decoder $D$. We can also pass the style to both inputs of $D$ to get $I_{ss}$ and similarly we get $I_{cc}$. Now we can use $\mathcal L_c$, $\mathcal L_s$, $\mathcal L_{id1}$,$\mathcal L_{id2}$ from \cite{ImageStyleTransformer}. We can then combine these into the final loss
\begin{align}
	\mathcal L &= \lambda_d \mathcal L_d + \lambda_c \mathcal L_c + \lambda_s \mathcal L_s + \lambda_{id1} \mathcal L_{id1} + \lambda_{id2} \mathcal L_{id2}
\end{align}
where the $\lambda$'s are weightings. We call this model D). 

We also define model E) where we modify (1) to
\begin{align}
	\mathcal L_d &= \sum_{I \in \{(I_s)^g_1, (I_s)^g_2\}} \sum_{I' \in V - \{ I \}} H(D(SE_t(I), CE(I_c)), D(SE_s(I'), CE(I_c))).
\end{align}
Where calculating the loss after the decoder should weight the most relevant student and teacher differences higher.

\section{Experiments}

\subsection{Implementation Details}

MS-COCO [cite] is used as the content dataset and WikiArt [cite] is used as the style dataset. In the training stage,
all the images are randomly cropped into a fixed resolution of 256 Ã— 256, while any image resolution is supported at the test time. We used the Adam optimizer [cite] and a warm-up adjustment strategy [cite] is used. Our GPU is not as big as the one used in the original paper, so while we could not train with their batch size, we modified the learning rate decay, number of iterations trained, and gradient accumulation steps to mimic the original's training process of a batch size of 8 for 160,000 iterations. Gradients are also clipped to a maximum norm of 10 to prevent exploding gradients.

\subsection{Qualitative Evaluation}
In both experiments A and B, we can see that as training progresses, the model gradually learns to render the content image to the style of the style image. But in both cases, certain details of the objects depicted in the content image are lost with experiment B being better at preserving features. For experiment A, we can see both the content and style losses decrease over time. Meanwhile, experiment B has a decrease style loss but its content loss first decreases and then increases before oscilating and staying overall the same.

Experiment C failed to learn style transfer. At the beginning of the training, it renders each content image with a color that roughly matches the style image, but over time, it started to copy the content image while completely ignoring the style image. After 300k iterations, the model abruptly started to output single-colored images regardless of what the content and style images are. As expected, the model's content loss decreases as it learns to copy the content images exactly and the style loss see no improvements.

During its training process, experiment D seems to be slowly learning to replicate the content image while not paying much attention to the style image. The resulting outputs started as poor-quality content images, but they slowly converged to the content images with some occasional color losses. We see the content loss slowly decreases as training progresses while the style loss oscilates and shows no signs of improvement.

Finally, experiment E also failed. In early stages of the training, the model renders the content with many colored vertical strips and many features were lost. Eventually, the model started to output single-color images. We see no improvements in the style loss while the content loss abruptly drops at around 250k epochs when the model outputs became mostly single-colored images, then the content loss starts to oscillate again.
\subsection{Comparisons with SOTA Methods}

\subsubsection{Timing Information}

\subsubsection{Qualitative Evaluation}

\subsubsection{Quantitative Evaluation}

\section{Conclusion}

\medskip

% Our current sources
\nocite{*}

\bibliography{bib}

\end{document}